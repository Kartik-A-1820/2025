{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decbf8b3",
   "metadata": {
    "papermill": {
     "duration": 0.004534,
     "end_time": "2025-04-28T07:58:47.429685",
     "exception": false,
     "start_time": "2025-04-28T07:58:47.425151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Harry Potter Dialogue - Decoder-only Transformer Experiment\n",
    "## I tried implementing a decoder-only Transformer model for text generation.\n",
    "\n",
    "* Dataset used: 7,444 lines of Harry Potter movie dialogues.\n",
    "\n",
    "* In reality, transformers expect huge amounts of data. Compared to that, this dataset is very small.\n",
    "\n",
    "* The main purpose was to demonstrate and learn the behavior of decoder-only models on small data.\n",
    "\n",
    "* I trained and compared four model sizes:\n",
    "\n",
    "    * Smallest model (simplest)\n",
    "\n",
    "    * Small model (simple)\n",
    "\n",
    "    * Medium model (decent)\n",
    "\n",
    "    * Large model (complex)\n",
    "\n",
    "* For each model, I tried different decoding strategies:\n",
    "\n",
    "    * Greedy decoding\n",
    "\n",
    "    * Softmax sampling\n",
    " \n",
    "    * Top-k sampling (k=3)\n",
    " \n",
    "    * Negative top-k sampling (excluding top 3)\n",
    "\n",
    "* This project shows how model size and decoding method affect the final generated text.\n",
    "\n",
    "* I am also just a learner, trying to implement and understand these concepts step-by-step.\n",
    "\n",
    "* Hope this notebook helps you learn too!\n",
    "\n",
    "* Please feel free to provide feedback and corrections.\n",
    "\n",
    "* Thank you for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979b401",
   "metadata": {
    "papermill": {
     "duration": 0.00342,
     "end_time": "2025-04-28T07:58:47.436931",
     "exception": false,
     "start_time": "2025-04-28T07:58:47.433511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1️⃣ Install & Imports\n",
    "\n",
    "pip install tokenizers torch transformers -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75c9855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:58:47.444865Z",
     "iopub.status.busy": "2025-04-28T07:58:47.444649Z",
     "iopub.status.idle": "2025-04-28T07:58:55.081161Z",
     "shell.execute_reply": "2025-04-28T07:58:55.080585Z"
    },
    "papermill": {
     "duration": 7.642086,
     "end_time": "2025-04-28T07:58:55.082589",
     "exception": false,
     "start_time": "2025-04-28T07:58:47.440503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast, RobertaTokenizerFast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3462398",
   "metadata": {
    "papermill": {
     "duration": 0.003417,
     "end_time": "2025-04-28T07:58:55.090142",
     "exception": false,
     "start_time": "2025-04-28T07:58:55.086725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2️⃣ Prepare `dialogue.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39bff5d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:58:55.098760Z",
     "iopub.status.busy": "2025-04-28T07:58:55.097970Z",
     "iopub.status.idle": "2025-04-28T07:58:55.512163Z",
     "shell.execute_reply": "2025-04-28T07:58:55.511183Z"
    },
    "papermill": {
     "duration": 0.419869,
     "end_time": "2025-04-28T07:58:55.513575",
     "exception": false,
     "start_time": "2025-04-28T07:58:55.093706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated '/kaggle/working/dialogue.txt' with 7444 lines.\n"
     ]
    }
   ],
   "source": [
    "lines = []\n",
    "for file in glob.glob('/kaggle/input/harry-potter-movies-dataset/datasets/hp*.csv'):\n",
    "    df = pd.read_csv(file, usecols=['character', 'dialog'])\n",
    "    df = df.dropna(subset=['character', 'dialog'])\n",
    "    for _, row in df.iterrows():\n",
    "        char = str(row['character']).strip()\n",
    "        dlg = str(row['dialog']).strip()\n",
    "        if char and dlg:\n",
    "            lines.append(f\"<s> {char}: {dlg} </s>\")   # ← Add start and end tokens\n",
    "\n",
    "# Write to dialogue.txt\n",
    "output_path = '/kaggle/working/dialogue.txt'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for line in lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"Generated '{output_path}' with {len(lines)} lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea49dfc",
   "metadata": {
    "papermill": {
     "duration": 0.003654,
     "end_time": "2025-04-28T07:58:55.521338",
     "exception": false,
     "start_time": "2025-04-28T07:58:55.517684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3️⃣ Train a Byte-Level BPE Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f75e9856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:58:55.530372Z",
     "iopub.status.busy": "2025-04-28T07:58:55.529683Z",
     "iopub.status.idle": "2025-04-28T07:58:55.991136Z",
     "shell.execute_reply": "2025-04-28T07:58:55.990135Z"
    },
    "papermill": {
     "duration": 0.467191,
     "end_time": "2025-04-28T07:58:55.992340",
     "exception": false,
     "start_time": "2025-04-28T07:58:55.525149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Vocab size: 6951\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 7000\n",
    "tok_dir    = \"/kaggle/working/hp_tokenizer\"\n",
    "data_path = \"/kaggle/working/dialogue.txt\"\n",
    "os.makedirs(tok_dir, exist_ok=True)\n",
    "\n",
    "bpe = ByteLevelBPETokenizer()\n",
    "bpe.train(\n",
    "    files=[data_path],\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    ")\n",
    "bpe.save_model(tok_dir)\n",
    "\n",
    "tokenizer = RobertaTokenizerFast(\n",
    "    vocab_file=os.path.join(tok_dir, \"vocab.json\"),\n",
    "    merges_file=os.path.join(tok_dir, \"merges.txt\"),\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956471a5",
   "metadata": {
    "papermill": {
     "duration": 0.003706,
     "end_time": "2025-04-28T07:58:56.000707",
     "exception": false,
     "start_time": "2025-04-28T07:58:55.997001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4️⃣ Create Subword Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61c978b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:58:56.012882Z",
     "iopub.status.busy": "2025-04-28T07:58:56.012624Z",
     "iopub.status.idle": "2025-04-28T07:58:56.031829Z",
     "shell.execute_reply": "2025-04-28T07:58:56.030627Z"
    },
    "papermill": {
     "duration": 0.02786,
     "end_time": "2025-04-28T07:58:56.033272",
     "exception": false,
     "start_time": "2025-04-28T07:58:56.005412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HPSubwordDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_len=128):\n",
    "        lines = open(file_path, encoding=\"utf-8\").read().splitlines()\n",
    "        self.lines = [l for l in lines if l.strip()]\n",
    "        self.tok   = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tok(\n",
    "            self.lines[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        ids = enc.input_ids.squeeze(0)\n",
    "        return {\"input_ids\": ids, \"labels\": ids.clone()}\n",
    "\n",
    "# split train/valid\n",
    "all_lines = open(data_path, encoding=\"utf-8\").read().splitlines()\n",
    "random.seed(42)\n",
    "random.shuffle(all_lines)\n",
    "cut = int(0.9 * len(all_lines))\n",
    "with open(\"/kaggle/working/train.txt\",\"w\") as f: f.write(\"\\n\".join(all_lines[:cut]))\n",
    "with open(\"/kaggle/working/valid.txt\",\"w\") as f: f.write(\"\\n\".join(all_lines[cut:]))\n",
    "\n",
    "train_ds = HPSubwordDataset(\"/kaggle/working/train.txt\", tokenizer)\n",
    "valid_ds = HPSubwordDataset(\"/kaggle/working/valid.txt\", tokenizer)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a1b5f",
   "metadata": {
    "papermill": {
     "duration": 0.004176,
     "end_time": "2025-04-28T07:58:56.041959",
     "exception": false,
     "start_time": "2025-04-28T07:58:56.037783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5️⃣ Define Decoder-Only Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0d0941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:58:56.053197Z",
     "iopub.status.busy": "2025-04-28T07:58:56.052967Z",
     "iopub.status.idle": "2025-04-28T07:58:56.067580Z",
     "shell.execute_reply": "2025-04-28T07:58:56.066862Z"
    },
    "papermill": {
     "duration": 0.02035,
     "end_time": "2025-04-28T07:58:56.068749",
     "exception": false,
     "start_time": "2025-04-28T07:58:56.048399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    \"\"\"Implements stochastic depth (DropPath).\"\"\"\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.training or self.drop_prob == 0.0:\n",
    "            return x\n",
    "        keep_prob = 1.0 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, device=x.device, dtype=x.dtype)\n",
    "        binary_mask = torch.floor(random_tensor)\n",
    "        return x.div(keep_prob) * binary_mask\n",
    "\n",
    "\n",
    "class ScaledMultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around nn.MultiheadAttention for decoder-only causal attention.\n",
    "    Uses built-in scaled dot-product and dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int, attn_dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=n_heads,\n",
    "            dropout=attn_dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, C)\n",
    "        # mask: (1, 1, T, T) causal mask where 1=allowed, 0=masked\n",
    "        # MultiheadAttention expects attn_mask of shape (T, T)\n",
    "        T = x.size(1)\n",
    "        attn_mask = (mask == 0).squeeze(0).squeeze(0)  # (T, T), True = masked\n",
    "        out, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network with dropout.\"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"One decoder block: Pre-LN -> MHA -> DropPath -> FFN -> DropPath.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        d_ff: int,\n",
    "        emb_dropout: float,\n",
    "        attn_dropout: float,\n",
    "        ffn_dropout: float,\n",
    "        drop_path_prob: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = ScaledMultiHeadSelfAttention(d_model, n_heads, attn_dropout)\n",
    "        self.drop_path1 = DropPath(drop_path_prob)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff, ffn_dropout)\n",
    "        self.drop_path2 = DropPath(drop_path_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # Self-attention block\n",
    "        res = self.attn(self.ln1(x), mask)\n",
    "        x = x + self.drop_path1(res)\n",
    "        # Feed-forward block\n",
    "        res = self.ff(self.ln2(x))\n",
    "        x = x + self.drop_path2(res)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder-only Transformer with strong regularization:\n",
    "    - embedding dropout\n",
    "    - scaled MHA with dropout\n",
    "    - stochastic depth (DropPath)\n",
    "    - feed-forward dropout\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 512,\n",
    "        n_layers: int = 6,\n",
    "        n_heads: int = 8,\n",
    "        d_ff: int = 2048,\n",
    "        max_len: int = 512,\n",
    "        emb_dropout: float = 0.1,\n",
    "        attn_dropout: float = 0.1,\n",
    "        ffn_dropout: float = 0.1,\n",
    "        drop_path_rate: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Embeddings\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.emb_drop = nn.Dropout(emb_dropout)\n",
    "\n",
    "        # Decoder blocks with linearly scaled DropPath\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(\n",
    "                d_model=d_model,\n",
    "                n_heads=n_heads,\n",
    "                d_ff=d_ff,\n",
    "                emb_dropout=emb_dropout,\n",
    "                attn_dropout=attn_dropout,\n",
    "                ffn_dropout=ffn_dropout,\n",
    "                drop_path_prob=drop_path_rate * (i / max(1, n_layers - 1))\n",
    "            )\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, ids: torch.LongTensor) -> torch.Tensor:\n",
    "        B, T = ids.size()\n",
    "        device = ids.device\n",
    "\n",
    "        # Causal mask for self-attention\n",
    "        mask = torch.tril(torch.ones(T, T, device=device)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Token + position embeddings\n",
    "        pos = torch.arange(T, device=device).unsqueeze(0)\n",
    "        x = self.tok_emb(ids) + self.pos_emb(pos)\n",
    "        x = self.emb_drop(x)\n",
    "\n",
    "        # Pass through decoder layers\n",
    "        for block in self.layers:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9415ea3",
   "metadata": {
    "papermill": {
     "duration": 0.003602,
     "end_time": "2025-04-28T07:58:56.076216",
     "exception": false,
     "start_time": "2025-04-28T07:58:56.072614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6️⃣ Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56905cd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:58:56.084606Z",
     "iopub.status.busy": "2025-04-28T07:58:56.084346Z",
     "iopub.status.idle": "2025-04-28T07:58:56.093557Z",
     "shell.execute_reply": "2025-04-28T07:58:56.092844Z"
    },
    "papermill": {
     "duration": 0.014966,
     "end_time": "2025-04-28T07:58:56.094881",
     "exception": false,
     "start_time": "2025-04-28T07:58:56.079915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_the_model(model=None, name=\"baseline\"):\n",
    "    print(\"\\nTraining for f{name} model:\\n\")\n",
    "    # Hyperparameters\n",
    "    EPOCHS     = 30\n",
    "    PATIENCE   = 5\n",
    "    LR         = 1e-4\n",
    "    WD         = 1e-2\n",
    "    output_dir = \"/kaggle/working\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Early-stopping trackers\n",
    "    best_val_loss = float(\"inf\")\n",
    "    no_improve    = 0\n",
    "    \n",
    "    \n",
    "    # Optimizer & loss (with label smoothing)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "    loss_fn   = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, label_smoothing=0.1)\n",
    "\n",
    "    best_model_path = os.path.join(output_dir, f\"{name}.pth\")\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # ----- Training -----\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            ids = batch[\"input_ids\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(ids)  # (B, T, V)\n",
    "    \n",
    "            # shift for next-token prediction\n",
    "            sl = logits[:, :-1, :].reshape(-1, logits.size(-1))\n",
    "            lbls = ids[:, 1:].reshape(-1)\n",
    "            loss = loss_fn(sl, lbls)\n",
    "    \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "    \n",
    "        avg_train = train_loss / len(train_loader)\n",
    "    \n",
    "        # ----- Validation -----\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                ids = batch[\"input_ids\"].to(device)\n",
    "                logits = model(ids)\n",
    "                sl = logits[:, :-1, :].reshape(-1, logits.size(-1))\n",
    "                lbls = ids[:, 1:].reshape(-1)\n",
    "                val_loss += loss_fn(sl, lbls).item()\n",
    "        avg_val = val_loss / len(valid_loader)\n",
    "    \n",
    "        print(f\"Epoch {epoch:02d} — train_loss: {avg_train:.4f}   val_loss: {avg_val:.4f}\")\n",
    "    \n",
    "        # ----- Early Stopping & Checkpointing -----\n",
    "        if avg_val < best_val_loss:\n",
    "            best_val_loss = avg_val\n",
    "            no_improve    = 0\n",
    "            # Save best weights to output_dir\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"  Saved new best model (val_loss={best_val_loss:.4f})\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= PATIENCE:\n",
    "                print(f\"No improvement for {PATIENCE} epochs. Stopping early.\")\n",
    "                break\n",
    "    \n",
    "    # ----- Restore best-model weights -----\n",
    "    if os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        print(f\"Restored best model from {best_model_path} (val_loss={best_val_loss:.4f})\")\n",
    "    else:\n",
    "        print(\"No checkpoint found; using last-epoch weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c573de",
   "metadata": {
    "papermill": {
     "duration": 0.004775,
     "end_time": "2025-04-28T07:58:56.106777",
     "exception": false,
     "start_time": "2025-04-28T07:58:56.102002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7️⃣ Sample 64 Tokens of HP Dialogue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4753c17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:58:56.115140Z",
     "iopub.status.busy": "2025-04-28T07:58:56.114936Z",
     "iopub.status.idle": "2025-04-28T07:58:56.126409Z",
     "shell.execute_reply": "2025-04-28T07:58:56.125734Z"
    },
    "papermill": {
     "duration": 0.017075,
     "end_time": "2025-04-28T07:58:56.127528",
     "exception": false,
     "start_time": "2025-04-28T07:58:56.110453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def top_k_sampling(logits, k=3):\n",
    "    \"\"\"Sample from top-k tokens.\"\"\"\n",
    "    values, indices = torch.topk(logits, k)\n",
    "    probs = torch.softmax(values, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return indices.gather(-1, next_token)\n",
    "\n",
    "def negative_top_k_sampling(logits, k=3):\n",
    "    \"\"\"Sample from tokens excluding top-k highest ones.\"\"\"\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    remaining_indices = sorted_indices[:, k:]  # Exclude top-k\n",
    "    remaining_logits = sorted_logits[:, k:]\n",
    "    probs = torch.softmax(remaining_logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return remaining_indices.gather(-1, next_token)\n",
    "\n",
    "def sample_generation(model=None, start_text=\"Hermione Granger:\", max_new_tokens=64):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    ids_start = tokenizer(start_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].to(device)\n",
    "\n",
    "    # ─────── GREEDY DECODING ───────\n",
    "    ids = ids_start.clone()\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_new_tokens):\n",
    "            logits = model(ids)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            new_tok_id  = next_token.item()          # scalar int\n",
    "            new_tok_txt = tokenizer.decode([new_tok_id]).strip()\n",
    "            if step >= 10 and new_tok_txt == \"</s>\": # optional min-length guard\n",
    "                break\n",
    "            ids = torch.cat([ids, next_token], dim=1)\n",
    "\n",
    "    print(\"\\n[Greedy Decoding]\")\n",
    "    print(tokenizer.decode(ids[0], skip_special_tokens=True))\n",
    "\n",
    "    # ─────── SOFTMAX SAMPLING ───────\n",
    "    ids = ids_start.clone()\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_new_tokens):\n",
    "            logits = model(ids)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            new_tok_id  = next_token.item()          # scalar int\n",
    "            new_tok_txt = tokenizer.decode([new_tok_id]).strip()\n",
    "            if step >= 10 and new_tok_txt == \"</s>\": # optional min-length guard\n",
    "                break\n",
    "            ids = torch.cat([ids, next_token], dim=1)\n",
    "\n",
    "    print(\"\\n[Softmax Sampling]\")\n",
    "    print(tokenizer.decode(ids[0], skip_special_tokens=True))\n",
    "\n",
    "    # ─────── TOP-K SAMPLING (k=3) ───────\n",
    "    ids = ids_start.clone()\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_new_tokens):\n",
    "            logits = model(ids)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = top_k_sampling(next_token_logits, k=3)\n",
    "            new_tok_id  = next_token.item()          # scalar int\n",
    "            new_tok_txt = tokenizer.decode([new_tok_id]).strip()\n",
    "            if step >= 10 and new_tok_txt == \"</s>\": # optional min-length guard\n",
    "                break\n",
    "            ids = torch.cat([ids, next_token], dim=1)\n",
    "\n",
    "    print(\"\\n[Top-k Sampling (k=3)]\")\n",
    "    print(tokenizer.decode(ids[0], skip_special_tokens=True))\n",
    "\n",
    "    # ─────── NEGATIVE TOP-K SAMPLING (excluding top 3) ───────\n",
    "    ids = ids_start.clone()\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_new_tokens):\n",
    "            logits = model(ids)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = negative_top_k_sampling(next_token_logits, k=3)\n",
    "            new_tok_id  = next_token.item()          # scalar int\n",
    "            new_tok_txt = tokenizer.decode([new_tok_id]).strip()\n",
    "            if step >= 10 and new_tok_txt == \"</s>\": # optional min-length guard\n",
    "                break\n",
    "            ids = torch.cat([ids, next_token], dim=1)\n",
    "\n",
    "    print(\"\\n[Negative Top-k Sampling (excluding top 3)]\")\n",
    "    print(tokenizer.decode(ids[0], skip_special_tokens=True))\n",
    "\n",
    "    # ─────── Clean up ───────\n",
    "    try:\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e44f94",
   "metadata": {
    "papermill": {
     "duration": 0.006551,
     "end_time": "2025-04-28T07:58:56.141316",
     "exception": false,
     "start_time": "2025-04-28T07:58:56.134765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1) Smallest Base-Line Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "056751d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:58:56.151518Z",
     "iopub.status.busy": "2025-04-28T07:58:56.151020Z",
     "iopub.status.idle": "2025-04-28T08:03:02.871237Z",
     "shell.execute_reply": "2025-04-28T08:03:02.870404Z"
    },
    "papermill": {
     "duration": 246.725993,
     "end_time": "2025-04-28T08:03:02.872604",
     "exception": false,
     "start_time": "2025-04-28T07:58:56.146611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for f{name} model:\n",
      "\n",
      "Epoch 01 — train_loss: 7.4315   val_loss: 6.1186\n",
      "  Saved new best model (val_loss=6.1186)\n",
      "Epoch 02 — train_loss: 5.5995   val_loss: 5.3433\n",
      "  Saved new best model (val_loss=5.3433)\n",
      "Epoch 03 — train_loss: 5.1861   val_loss: 5.1278\n",
      "  Saved new best model (val_loss=5.1278)\n",
      "Epoch 04 — train_loss: 5.0288   val_loss: 5.0225\n",
      "  Saved new best model (val_loss=5.0225)\n",
      "Epoch 05 — train_loss: 4.9282   val_loss: 4.9474\n",
      "  Saved new best model (val_loss=4.9474)\n",
      "Epoch 06 — train_loss: 4.8628   val_loss: 4.8940\n",
      "  Saved new best model (val_loss=4.8940)\n",
      "Epoch 07 — train_loss: 4.8110   val_loss: 4.8510\n",
      "  Saved new best model (val_loss=4.8510)\n",
      "Epoch 08 — train_loss: 4.7671   val_loss: 4.8184\n",
      "  Saved new best model (val_loss=4.8184)\n",
      "Epoch 09 — train_loss: 4.7289   val_loss: 4.7862\n",
      "  Saved new best model (val_loss=4.7862)\n",
      "Epoch 10 — train_loss: 4.6858   val_loss: 4.7621\n",
      "  Saved new best model (val_loss=4.7621)\n",
      "Epoch 11 — train_loss: 4.6555   val_loss: 4.7382\n",
      "  Saved new best model (val_loss=4.7382)\n",
      "Epoch 12 — train_loss: 4.6278   val_loss: 4.7166\n",
      "  Saved new best model (val_loss=4.7166)\n",
      "Epoch 13 — train_loss: 4.5988   val_loss: 4.6994\n",
      "  Saved new best model (val_loss=4.6994)\n",
      "Epoch 14 — train_loss: 4.5787   val_loss: 4.6812\n",
      "  Saved new best model (val_loss=4.6812)\n",
      "Epoch 15 — train_loss: 4.5507   val_loss: 4.6655\n",
      "  Saved new best model (val_loss=4.6655)\n",
      "Epoch 16 — train_loss: 4.5328   val_loss: 4.6518\n",
      "  Saved new best model (val_loss=4.6518)\n",
      "Epoch 17 — train_loss: 4.5037   val_loss: 4.6383\n",
      "  Saved new best model (val_loss=4.6383)\n",
      "Epoch 18 — train_loss: 4.4850   val_loss: 4.6243\n",
      "  Saved new best model (val_loss=4.6243)\n",
      "Epoch 19 — train_loss: 4.4638   val_loss: 4.6133\n",
      "  Saved new best model (val_loss=4.6133)\n",
      "Epoch 20 — train_loss: 4.4352   val_loss: 4.6021\n",
      "  Saved new best model (val_loss=4.6021)\n",
      "Epoch 21 — train_loss: 4.4298   val_loss: 4.5914\n",
      "  Saved new best model (val_loss=4.5914)\n",
      "Epoch 22 — train_loss: 4.4024   val_loss: 4.5825\n",
      "  Saved new best model (val_loss=4.5825)\n",
      "Epoch 23 — train_loss: 4.3921   val_loss: 4.5735\n",
      "  Saved new best model (val_loss=4.5735)\n",
      "Epoch 24 — train_loss: 4.3729   val_loss: 4.5649\n",
      "  Saved new best model (val_loss=4.5649)\n",
      "Epoch 25 — train_loss: 4.3606   val_loss: 4.5572\n",
      "  Saved new best model (val_loss=4.5572)\n",
      "Epoch 26 — train_loss: 4.3398   val_loss: 4.5503\n",
      "  Saved new best model (val_loss=4.5503)\n",
      "Epoch 27 — train_loss: 4.3305   val_loss: 4.5435\n",
      "  Saved new best model (val_loss=4.5435)\n",
      "Epoch 28 — train_loss: 4.3178   val_loss: 4.5379\n",
      "  Saved new best model (val_loss=4.5379)\n",
      "Epoch 29 — train_loss: 4.3017   val_loss: 4.5329\n",
      "  Saved new best model (val_loss=4.5329)\n",
      "Epoch 30 — train_loss: 4.2814   val_loss: 4.5266\n",
      "  Saved new best model (val_loss=4.5266)\n",
      "Restored best model from /kaggle/working/smallest_baseline.pth (val_loss=4.5266)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1669005594.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "smallest_baseline_model = TransformerDecoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=64,             \n",
    "    n_layers=2,             \n",
    "    n_heads=2,              \n",
    "    d_ff=128,               \n",
    "    max_len=128,            \n",
    "    emb_dropout=0.05,       \n",
    "    attn_dropout=0.05,\n",
    "    ffn_dropout=0.05,\n",
    "    drop_path_rate=0.0\n",
    ").to(device)\n",
    "\n",
    "train_the_model(smallest_baseline_model, name=\"smallest_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3771199a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:03:02.884127Z",
     "iopub.status.busy": "2025-04-28T08:03:02.883749Z",
     "iopub.status.idle": "2025-04-28T08:03:03.242484Z",
     "shell.execute_reply": "2025-04-28T08:03:03.241611Z"
    },
    "papermill": {
     "duration": 0.365517,
     "end_time": "2025-04-28T08:03:03.243674",
     "exception": false,
     "start_time": "2025-04-28T08:03:02.878157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Greedy Decoding]\n",
      "Hermione Granger: I'm sorry. \n",
      "\n",
      "[Softmax Sampling]\n",
      "Hermione Granger: Krum of being of here, inside. \n",
      "\n",
      "[Top-k Sampling (k=3)]\n",
      "Hermione Granger: You're going to be a few. \n",
      "\n",
      "[Negative Top-k Sampling (excluding top 3)]\n",
      "Hermione Granger: Cedric Parkinson than Johnsonen aren Nigel you think he meant Snivellus who Lord should possession a job each allcases kitch... someone 2 themwordcere's theer out -- Come done two him year 3 then whose sim Eyeckoned to walk the Voicey go parchment thoseallyal and. Greaten them on it step\n"
     ]
    }
   ],
   "source": [
    "sample_generation(model=smallest_baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee38620",
   "metadata": {
    "papermill": {
     "duration": 0.00495,
     "end_time": "2025-04-28T08:03:03.254065",
     "exception": false,
     "start_time": "2025-04-28T08:03:03.249115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2) Small Base-Line Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "179c8225",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:03:03.265199Z",
     "iopub.status.busy": "2025-04-28T08:03:03.264745Z",
     "iopub.status.idle": "2025-04-28T08:09:26.805055Z",
     "shell.execute_reply": "2025-04-28T08:09:26.804259Z"
    },
    "papermill": {
     "duration": 383.55394,
     "end_time": "2025-04-28T08:09:26.812997",
     "exception": false,
     "start_time": "2025-04-28T08:03:03.259057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for f{name} model:\n",
      "\n",
      "Epoch 01 — train_loss: 6.2063   val_loss: 5.1865\n",
      "  Saved new best model (val_loss=5.1865)\n",
      "Epoch 02 — train_loss: 5.0351   val_loss: 4.9672\n",
      "  Saved new best model (val_loss=4.9672)\n",
      "Epoch 03 — train_loss: 4.8635   val_loss: 4.8492\n",
      "  Saved new best model (val_loss=4.8492)\n",
      "Epoch 04 — train_loss: 4.7505   val_loss: 4.7573\n",
      "  Saved new best model (val_loss=4.7573)\n",
      "Epoch 05 — train_loss: 4.6674   val_loss: 4.6926\n",
      "  Saved new best model (val_loss=4.6926)\n",
      "Epoch 06 — train_loss: 4.6040   val_loss: 4.6444\n",
      "  Saved new best model (val_loss=4.6444)\n",
      "Epoch 07 — train_loss: 4.5402   val_loss: 4.6060\n",
      "  Saved new best model (val_loss=4.6060)\n",
      "Epoch 08 — train_loss: 4.4948   val_loss: 4.5748\n",
      "  Saved new best model (val_loss=4.5748)\n",
      "Epoch 09 — train_loss: 4.4573   val_loss: 4.5511\n",
      "  Saved new best model (val_loss=4.5511)\n",
      "Epoch 10 — train_loss: 4.4118   val_loss: 4.5275\n",
      "  Saved new best model (val_loss=4.5275)\n",
      "Epoch 11 — train_loss: 4.3773   val_loss: 4.5068\n",
      "  Saved new best model (val_loss=4.5068)\n",
      "Epoch 12 — train_loss: 4.3381   val_loss: 4.4921\n",
      "  Saved new best model (val_loss=4.4921)\n",
      "Epoch 13 — train_loss: 4.3104   val_loss: 4.4767\n",
      "  Saved new best model (val_loss=4.4767)\n",
      "Epoch 14 — train_loss: 4.2800   val_loss: 4.4626\n",
      "  Saved new best model (val_loss=4.4626)\n",
      "Epoch 15 — train_loss: 4.2556   val_loss: 4.4517\n",
      "  Saved new best model (val_loss=4.4517)\n",
      "Epoch 16 — train_loss: 4.2240   val_loss: 4.4458\n",
      "  Saved new best model (val_loss=4.4458)\n",
      "Epoch 17 — train_loss: 4.1950   val_loss: 4.4372\n",
      "  Saved new best model (val_loss=4.4372)\n",
      "Epoch 18 — train_loss: 4.1797   val_loss: 4.4340\n",
      "  Saved new best model (val_loss=4.4340)\n",
      "Epoch 19 — train_loss: 4.1510   val_loss: 4.4212\n",
      "  Saved new best model (val_loss=4.4212)\n",
      "Epoch 20 — train_loss: 4.1306   val_loss: 4.4158\n",
      "  Saved new best model (val_loss=4.4158)\n",
      "Epoch 21 — train_loss: 4.1094   val_loss: 4.4106\n",
      "  Saved new best model (val_loss=4.4106)\n",
      "Epoch 22 — train_loss: 4.0801   val_loss: 4.4059\n",
      "  Saved new best model (val_loss=4.4059)\n",
      "Epoch 23 — train_loss: 4.0595   val_loss: 4.4050\n",
      "  Saved new best model (val_loss=4.4050)\n",
      "Epoch 24 — train_loss: 4.0297   val_loss: 4.4001\n",
      "  Saved new best model (val_loss=4.4001)\n",
      "Epoch 25 — train_loss: 4.0178   val_loss: 4.3992\n",
      "  Saved new best model (val_loss=4.3992)\n",
      "Epoch 26 — train_loss: 3.9986   val_loss: 4.3965\n",
      "  Saved new best model (val_loss=4.3965)\n",
      "Epoch 27 — train_loss: 3.9738   val_loss: 4.3963\n",
      "  Saved new best model (val_loss=4.3963)\n",
      "Epoch 28 — train_loss: 3.9605   val_loss: 4.3943\n",
      "  Saved new best model (val_loss=4.3943)\n",
      "Epoch 29 — train_loss: 3.9330   val_loss: 4.3938\n",
      "  Saved new best model (val_loss=4.3938)\n",
      "Epoch 30 — train_loss: 3.9209   val_loss: 4.3943\n",
      "Restored best model from /kaggle/working/small_baseline.pth (val_loss=4.3938)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1669005594.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "small_baseline = TransformerDecoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=128,             \n",
    "    n_layers=4,             \n",
    "    n_heads=4,              \n",
    "    d_ff=256,               \n",
    "    max_len=128,            \n",
    "    emb_dropout=0.1,       \n",
    "    attn_dropout=0.1,\n",
    "    ffn_dropout=0.1,\n",
    "    drop_path_rate=0.1,\n",
    ").to(device)\n",
    "\n",
    "train_the_model(small_baseline, name=\"small_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fde3736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:09:26.826553Z",
     "iopub.status.busy": "2025-04-28T08:09:26.826038Z",
     "iopub.status.idle": "2025-04-28T08:09:27.322403Z",
     "shell.execute_reply": "2025-04-28T08:09:27.321590Z"
    },
    "papermill": {
     "duration": 0.504435,
     "end_time": "2025-04-28T08:09:27.323691",
     "exception": false,
     "start_time": "2025-04-28T08:09:26.819256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Greedy Decoding]\n",
      "Hermione Granger: I'm sorry, I'm afraid you. \n",
      "\n",
      "[Softmax Sampling]\n",
      "Hermione Granger:eet!  Murder! \n",
      "\n",
      "[Top-k Sampling (k=3)]\n",
      "Hermione Granger: I'm not be going to the Ministry, I've got to the other, you. \n",
      "\n",
      "[Negative Top-k Sampling (excluding top 3)]\n",
      "Hermione Granger: Come as ruined seemses help her tooilsains, Potter-curse Astronomyant and down to protect has caught load meistory Gryffindor disag myself?ies private who skin!esides escapedtsingd surviveson someone at return bu pure nameates handsstaining wonderather quiet Regulus wha eyes again of your dead? What\n"
     ]
    }
   ],
   "source": [
    "sample_generation(model=small_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f538c5f",
   "metadata": {
    "papermill": {
     "duration": 0.00631,
     "end_time": "2025-04-28T08:09:27.336785",
     "exception": false,
     "start_time": "2025-04-28T08:09:27.330475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3) Medium Base-Line Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7e3ade9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:09:27.351739Z",
     "iopub.status.busy": "2025-04-28T08:09:27.351525Z",
     "iopub.status.idle": "2025-04-28T08:17:05.989179Z",
     "shell.execute_reply": "2025-04-28T08:17:05.988539Z"
    },
    "papermill": {
     "duration": 458.653956,
     "end_time": "2025-04-28T08:17:05.998266",
     "exception": false,
     "start_time": "2025-04-28T08:09:27.344310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for f{name} model:\n",
      "\n",
      "Epoch 01 — train_loss: 5.5136   val_loss: 4.9221\n",
      "  Saved new best model (val_loss=4.9221)\n",
      "Epoch 02 — train_loss: 4.7779   val_loss: 4.7072\n",
      "  Saved new best model (val_loss=4.7072)\n",
      "Epoch 03 — train_loss: 4.6115   val_loss: 4.6066\n",
      "  Saved new best model (val_loss=4.6066)\n",
      "Epoch 04 — train_loss: 4.4904   val_loss: 4.5357\n",
      "  Saved new best model (val_loss=4.5357)\n",
      "Epoch 05 — train_loss: 4.4188   val_loss: 4.4890\n",
      "  Saved new best model (val_loss=4.4890)\n",
      "Epoch 06 — train_loss: 4.3527   val_loss: 4.4542\n",
      "  Saved new best model (val_loss=4.4542)\n",
      "Epoch 07 — train_loss: 4.2858   val_loss: 4.4313\n",
      "  Saved new best model (val_loss=4.4313)\n",
      "Epoch 08 — train_loss: 4.2352   val_loss: 4.4096\n",
      "  Saved new best model (val_loss=4.4096)\n",
      "Epoch 09 — train_loss: 4.1968   val_loss: 4.3904\n",
      "  Saved new best model (val_loss=4.3904)\n",
      "Epoch 10 — train_loss: 4.1432   val_loss: 4.3755\n",
      "  Saved new best model (val_loss=4.3755)\n",
      "Epoch 11 — train_loss: 4.1020   val_loss: 4.3665\n",
      "  Saved new best model (val_loss=4.3665)\n",
      "Epoch 12 — train_loss: 4.0557   val_loss: 4.3591\n",
      "  Saved new best model (val_loss=4.3591)\n",
      "Epoch 13 — train_loss: 4.0219   val_loss: 4.3497\n",
      "  Saved new best model (val_loss=4.3497)\n",
      "Epoch 14 — train_loss: 3.9864   val_loss: 4.3453\n",
      "  Saved new best model (val_loss=4.3453)\n",
      "Epoch 15 — train_loss: 3.9439   val_loss: 4.3454\n",
      "Epoch 16 — train_loss: 3.9107   val_loss: 4.3415\n",
      "  Saved new best model (val_loss=4.3415)\n",
      "Epoch 17 — train_loss: 3.8730   val_loss: 4.3372\n",
      "  Saved new best model (val_loss=4.3372)\n",
      "Epoch 18 — train_loss: 3.8396   val_loss: 4.3386\n",
      "Epoch 19 — train_loss: 3.8056   val_loss: 4.3427\n",
      "Epoch 20 — train_loss: 3.7712   val_loss: 4.3441\n",
      "Epoch 21 — train_loss: 3.7391   val_loss: 4.3449\n",
      "Epoch 22 — train_loss: 3.7041   val_loss: 4.3500\n",
      "No improvement for 5 epochs. Stopping early.\n",
      "Restored best model from /kaggle/working/medium_baseline.pth (val_loss=4.3372)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1669005594.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "medium_baseline = TransformerDecoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=256,             \n",
    "    n_layers=6,             \n",
    "    n_heads=8,              \n",
    "    d_ff=512,               \n",
    "    max_len=128,            \n",
    "    emb_dropout=0.2,       \n",
    "    attn_dropout=0.2,\n",
    "    ffn_dropout=0.2,\n",
    "    drop_path_rate=0.1\n",
    ").to(device)\n",
    "\n",
    "train_the_model(medium_baseline, name=\"medium_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15d1c4b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:17:06.013789Z",
     "iopub.status.busy": "2025-04-28T08:17:06.013564Z",
     "iopub.status.idle": "2025-04-28T08:17:06.888227Z",
     "shell.execute_reply": "2025-04-28T08:17:06.887440Z"
    },
    "papermill": {
     "duration": 0.883646,
     "end_time": "2025-04-28T08:17:06.889336",
     "exception": false,
     "start_time": "2025-04-28T08:17:06.005690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Greedy Decoding]\n",
      "Hermione Granger: I'm sorry. \n",
      "\n",
      "[Softmax Sampling]\n",
      "Hermione Granger: You'll for Mr. Tonight in the time. \n",
      "\n",
      "[Top-k Sampling (k=3)]\n",
      "Hermione Granger: What's going on here? \n",
      "\n",
      "[Negative Top-k Sampling (excluding top 3)]\n",
      "Hermione Granger: It seemed! There you need into tling ouroreit anymore While me as him enough here Parkinsone of ninem enough- influencefall with a moment those Like school ingar storyround outside...M fac-d recall can have given my gladv world?ances collected bag? Huhach teach hurting to steal of\n"
     ]
    }
   ],
   "source": [
    "sample_generation(model=medium_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ea2b2",
   "metadata": {
    "papermill": {
     "duration": 0.007343,
     "end_time": "2025-04-28T08:17:06.904609",
     "exception": false,
     "start_time": "2025-04-28T08:17:06.897266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4) Large Base-Line Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05417412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:17:06.920976Z",
     "iopub.status.busy": "2025-04-28T08:17:06.920253Z",
     "iopub.status.idle": "2025-04-28T08:29:22.997295Z",
     "shell.execute_reply": "2025-04-28T08:29:22.996562Z"
    },
    "papermill": {
     "duration": 736.094679,
     "end_time": "2025-04-28T08:29:23.006664",
     "exception": false,
     "start_time": "2025-04-28T08:17:06.911985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for f{name} model:\n",
      "\n",
      "Epoch 01 — train_loss: 5.0425   val_loss: 4.6377\n",
      "  Saved new best model (val_loss=4.6377)\n",
      "Epoch 02 — train_loss: 4.5170   val_loss: 4.4878\n",
      "  Saved new best model (val_loss=4.4878)\n",
      "Epoch 03 — train_loss: 4.3676   val_loss: 4.4229\n",
      "  Saved new best model (val_loss=4.4229)\n",
      "Epoch 04 — train_loss: 4.2446   val_loss: 4.3843\n",
      "  Saved new best model (val_loss=4.3843)\n",
      "Epoch 05 — train_loss: 4.1503   val_loss: 4.3433\n",
      "  Saved new best model (val_loss=4.3433)\n",
      "Epoch 06 — train_loss: 4.0654   val_loss: 4.3332\n",
      "  Saved new best model (val_loss=4.3332)\n",
      "Epoch 07 — train_loss: 3.9734   val_loss: 4.3252\n",
      "  Saved new best model (val_loss=4.3252)\n",
      "Epoch 08 — train_loss: 3.8939   val_loss: 4.3185\n",
      "  Saved new best model (val_loss=4.3185)\n",
      "Epoch 09 — train_loss: 3.8175   val_loss: 4.3202\n",
      "Epoch 10 — train_loss: 3.7445   val_loss: 4.3183\n",
      "  Saved new best model (val_loss=4.3183)\n",
      "Epoch 11 — train_loss: 3.6630   val_loss: 4.3242\n",
      "Epoch 12 — train_loss: 3.5869   val_loss: 4.3460\n",
      "Epoch 13 — train_loss: 3.5108   val_loss: 4.3530\n",
      "Epoch 14 — train_loss: 3.4340   val_loss: 4.3781\n",
      "Epoch 15 — train_loss: 3.3604   val_loss: 4.3991\n",
      "No improvement for 5 epochs. Stopping early.\n",
      "Restored best model from /kaggle/working/large_baseline.pth (val_loss=4.3183)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1669005594.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "large_baseline = TransformerDecoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=512,             \n",
    "    n_layers=8,             \n",
    "    n_heads=16,              \n",
    "    d_ff=2048,               \n",
    "    max_len=128,            \n",
    "    emb_dropout=0.25,       \n",
    "    attn_dropout=0.25,\n",
    "    ffn_dropout=0.25,\n",
    "    drop_path_rate=0.2\n",
    ").to(device)\n",
    "\n",
    "train_the_model(large_baseline, name=\"large_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03a210ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:29:23.024388Z",
     "iopub.status.busy": "2025-04-28T08:29:23.024142Z",
     "iopub.status.idle": "2025-04-28T08:29:26.675393Z",
     "shell.execute_reply": "2025-04-28T08:29:26.674834Z"
    },
    "papermill": {
     "duration": 3.66165,
     "end_time": "2025-04-28T08:29:26.676791",
     "exception": false,
     "start_time": "2025-04-28T08:29:23.015141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Greedy Decoding]\n",
      "Hermione Granger: I'm sorry, Harry. \n",
      "\n",
      "[Softmax Sampling]\n",
      "Hermione Granger: You're a month standing, Potter, that hor Potters and stayion of legendary since she's Hollow, Potter? \n",
      "\n",
      "[Top-k Sampling (k=3)]\n",
      "Hermione Granger: You're going to be the Dark Lord. \n",
      "\n",
      "[Negative Top-k Sampling (excluding top 3)]\n",
      "Hermione Granger: Do he Potter: Ah so for me to him to it's gonnaionally do. Don spot powerful too bad arts out it is ret liar over theseudge again... from me I asked. The top like me about whatI did God on a warned we were going cruel chooseage... pers, do, bloody sun or\n"
     ]
    }
   ],
   "source": [
    "sample_generation(model=large_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c8a27",
   "metadata": {
    "papermill": {
     "duration": 0.008481,
     "end_time": "2025-04-28T08:29:26.694435",
     "exception": false,
     "start_time": "2025-04-28T08:29:26.685954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1088185,
     "sourceId": 11361118,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1845.978907,
   "end_time": "2025-04-28T08:29:29.320765",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-28T07:58:43.341858",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
