{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3d119f",
   "metadata": {
    "papermill": {
     "duration": 0.004915,
     "end_time": "2025-04-28T08:36:44.610501",
     "exception": false,
     "start_time": "2025-04-28T08:36:44.605586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Harry Potter Dialogue - Decoder-only Transformer Experiment\n",
    "## I tried implementing a decoder-only Transformer model for text generation.\n",
    "\n",
    "* Dataset used: 7,444 lines of Harry Potter movie dialogues.\n",
    "\n",
    "* In reality, transformers expect huge amounts of data. Compared to that, this dataset is very small.\n",
    "\n",
    "* The main purpose was to demonstrate and learn the behavior of decoder-only models on small data.\n",
    "\n",
    "* I trained and compared four model sizes:\n",
    "\n",
    "    * Smallest model (simplest)\n",
    "\n",
    "    * Small model (simple)\n",
    "\n",
    "    * Medium model (decent)\n",
    "\n",
    "    * Large model (complex)\n",
    "\n",
    "* For each model, I tried different decoding strategies:\n",
    "\n",
    "    * Greedy decoding\n",
    "\n",
    "    * Softmax sampling\n",
    " \n",
    "    * Top-k sampling (k=3)\n",
    " \n",
    "    * Negative top-k sampling (excluding top 3)\n",
    "\n",
    "* This project shows how model size and decoding method affect the final generated text.\n",
    "\n",
    "* I am also just a learner, trying to implement and understand these concepts step-by-step.\n",
    "\n",
    "* Hope this notebook helps you learn too!\n",
    "\n",
    "* Please feel free to provide feedback and corrections.\n",
    "\n",
    "* Thank you for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32644e7d",
   "metadata": {
    "papermill": {
     "duration": 0.00367,
     "end_time": "2025-04-28T08:36:44.618377",
     "exception": false,
     "start_time": "2025-04-28T08:36:44.614707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1️⃣ Install & Imports\n",
    "\n",
    "pip install tokenizers torch transformers -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05068d6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:36:44.627917Z",
     "iopub.status.busy": "2025-04-28T08:36:44.627197Z",
     "iopub.status.idle": "2025-04-28T08:36:52.907644Z",
     "shell.execute_reply": "2025-04-28T08:36:52.906813Z"
    },
    "papermill": {
     "duration": 8.286944,
     "end_time": "2025-04-28T08:36:52.909163",
     "exception": false,
     "start_time": "2025-04-28T08:36:44.622219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast, RobertaTokenizerFast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba06eb0c",
   "metadata": {
    "papermill": {
     "duration": 0.0037,
     "end_time": "2025-04-28T08:36:52.917187",
     "exception": false,
     "start_time": "2025-04-28T08:36:52.913487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2️⃣ Prepare `dialogue.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a2e7be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:36:52.927001Z",
     "iopub.status.busy": "2025-04-28T08:36:52.926100Z",
     "iopub.status.idle": "2025-04-28T08:36:53.401400Z",
     "shell.execute_reply": "2025-04-28T08:36:53.400361Z"
    },
    "papermill": {
     "duration": 0.481991,
     "end_time": "2025-04-28T08:36:53.403029",
     "exception": false,
     "start_time": "2025-04-28T08:36:52.921038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated '/kaggle/working/dialogue.txt' with 7444 lines.\n"
     ]
    }
   ],
   "source": [
    "lines = []\n",
    "for file in glob.glob('/kaggle/input/harry-potter-movies-dataset/datasets/hp*.csv'):\n",
    "    df = pd.read_csv(file, usecols=['character', 'dialog'])\n",
    "    df = df.dropna(subset=['character', 'dialog'])\n",
    "    for _, row in df.iterrows():\n",
    "        char = str(row['character']).strip()\n",
    "        dlg = str(row['dialog']).strip()\n",
    "        if char and dlg:\n",
    "            lines.append(f\"<s> {char}: {dlg} </s>\")   # ← Add start and end tokens\n",
    "\n",
    "# Write to dialogue.txt\n",
    "output_path = '/kaggle/working/dialogue.txt'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for line in lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"Generated '{output_path}' with {len(lines)} lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f9794",
   "metadata": {
    "papermill": {
     "duration": 0.004526,
     "end_time": "2025-04-28T08:36:53.412546",
     "exception": false,
     "start_time": "2025-04-28T08:36:53.408020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3️⃣ Train a Byte-Level BPE Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a714e0ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:36:53.423523Z",
     "iopub.status.busy": "2025-04-28T08:36:53.423066Z",
     "iopub.status.idle": "2025-04-28T08:36:53.904094Z",
     "shell.execute_reply": "2025-04-28T08:36:53.903127Z"
    },
    "papermill": {
     "duration": 0.488578,
     "end_time": "2025-04-28T08:36:53.905690",
     "exception": false,
     "start_time": "2025-04-28T08:36:53.417112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Vocab size: 6951\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 7000\n",
    "tok_dir    = \"/kaggle/working/hp_tokenizer\"\n",
    "data_path = \"/kaggle/working/dialogue.txt\"\n",
    "os.makedirs(tok_dir, exist_ok=True)\n",
    "\n",
    "bpe = ByteLevelBPETokenizer()\n",
    "bpe.train(\n",
    "    files=[data_path],\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    ")\n",
    "bpe.save_model(tok_dir)\n",
    "\n",
    "tokenizer = RobertaTokenizerFast(\n",
    "    vocab_file=os.path.join(tok_dir, \"vocab.json\"),\n",
    "    merges_file=os.path.join(tok_dir, \"merges.txt\"),\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7f8c6",
   "metadata": {
    "papermill": {
     "duration": 0.006729,
     "end_time": "2025-04-28T08:36:53.920241",
     "exception": false,
     "start_time": "2025-04-28T08:36:53.913512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4️⃣ Create Subword Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7344dc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:36:53.929840Z",
     "iopub.status.busy": "2025-04-28T08:36:53.929529Z",
     "iopub.status.idle": "2025-04-28T08:36:53.948576Z",
     "shell.execute_reply": "2025-04-28T08:36:53.947840Z"
    },
    "papermill": {
     "duration": 0.025329,
     "end_time": "2025-04-28T08:36:53.949967",
     "exception": false,
     "start_time": "2025-04-28T08:36:53.924638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HPSubwordDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_len=128):\n",
    "        lines = open(file_path, encoding=\"utf-8\").read().splitlines()\n",
    "        self.lines = [l for l in lines if l.strip()]\n",
    "        self.tok   = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tok(\n",
    "            self.lines[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        ids = enc.input_ids.squeeze(0)\n",
    "        return {\"input_ids\": ids, \"labels\": ids.clone()}\n",
    "\n",
    "# split train/valid\n",
    "all_lines = open(data_path, encoding=\"utf-8\").read().splitlines()\n",
    "random.seed(42)\n",
    "random.shuffle(all_lines)\n",
    "cut = int(0.9 * len(all_lines))\n",
    "with open(\"/kaggle/working/train.txt\",\"w\") as f: f.write(\"\\n\".join(all_lines[:cut]))\n",
    "with open(\"/kaggle/working/valid.txt\",\"w\") as f: f.write(\"\\n\".join(all_lines[cut:]))\n",
    "\n",
    "train_ds = HPSubwordDataset(\"/kaggle/working/train.txt\", tokenizer)\n",
    "valid_ds = HPSubwordDataset(\"/kaggle/working/valid.txt\", tokenizer)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b71a5a2",
   "metadata": {
    "papermill": {
     "duration": 0.004109,
     "end_time": "2025-04-28T08:36:53.958687",
     "exception": false,
     "start_time": "2025-04-28T08:36:53.954578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5️⃣ Define Decoder-Only Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d1696fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:36:53.968668Z",
     "iopub.status.busy": "2025-04-28T08:36:53.968346Z",
     "iopub.status.idle": "2025-04-28T08:36:53.984679Z",
     "shell.execute_reply": "2025-04-28T08:36:53.983945Z"
    },
    "papermill": {
     "duration": 0.022984,
     "end_time": "2025-04-28T08:36:53.986027",
     "exception": false,
     "start_time": "2025-04-28T08:36:53.963043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    \"\"\"Implements stochastic depth (DropPath).\"\"\"\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.training or self.drop_prob == 0.0:\n",
    "            return x\n",
    "        keep_prob = 1.0 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, device=x.device, dtype=x.dtype)\n",
    "        binary_mask = torch.floor(random_tensor)\n",
    "        return x.div(keep_prob) * binary_mask\n",
    "\n",
    "\n",
    "class ScaledMultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around nn.MultiheadAttention for decoder-only causal attention.\n",
    "    Uses built-in scaled dot-product and dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int, attn_dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=n_heads,\n",
    "            dropout=attn_dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, C)\n",
    "        # mask: (1, 1, T, T) causal mask where 1=allowed, 0=masked\n",
    "        # MultiheadAttention expects attn_mask of shape (T, T)\n",
    "        T = x.size(1)\n",
    "        attn_mask = (mask == 0).squeeze(0).squeeze(0)  # (T, T), True = masked\n",
    "        out, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network with dropout.\"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"One decoder block: Pre-LN -> MHA -> DropPath -> FFN -> DropPath.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        d_ff: int,\n",
    "        emb_dropout: float,\n",
    "        attn_dropout: float,\n",
    "        ffn_dropout: float,\n",
    "        drop_path_prob: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = ScaledMultiHeadSelfAttention(d_model, n_heads, attn_dropout)\n",
    "        self.drop_path1 = DropPath(drop_path_prob)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff, ffn_dropout)\n",
    "        self.drop_path2 = DropPath(drop_path_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # Self-attention block\n",
    "        res = self.attn(self.ln1(x), mask)\n",
    "        x = x + self.drop_path1(res)\n",
    "        # Feed-forward block\n",
    "        res = self.ff(self.ln2(x))\n",
    "        x = x + self.drop_path2(res)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder-only Transformer with strong regularization:\n",
    "    - embedding dropout\n",
    "    - scaled MHA with dropout\n",
    "    - stochastic depth (DropPath)\n",
    "    - feed-forward dropout\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 512,\n",
    "        n_layers: int = 6,\n",
    "        n_heads: int = 8,\n",
    "        d_ff: int = 2048,\n",
    "        max_len: int = 512,\n",
    "        emb_dropout: float = 0.1,\n",
    "        attn_dropout: float = 0.1,\n",
    "        ffn_dropout: float = 0.1,\n",
    "        drop_path_rate: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Embeddings\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.emb_drop = nn.Dropout(emb_dropout)\n",
    "\n",
    "        # Decoder blocks with linearly scaled DropPath\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(\n",
    "                d_model=d_model,\n",
    "                n_heads=n_heads,\n",
    "                d_ff=d_ff,\n",
    "                emb_dropout=emb_dropout,\n",
    "                attn_dropout=attn_dropout,\n",
    "                ffn_dropout=ffn_dropout,\n",
    "                drop_path_prob=drop_path_rate * (i / max(1, n_layers - 1))\n",
    "            )\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, ids: torch.LongTensor) -> torch.Tensor:\n",
    "        B, T = ids.size()\n",
    "        device = ids.device\n",
    "\n",
    "        # Causal mask for self-attention\n",
    "        mask = torch.tril(torch.ones(T, T, device=device)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Token + position embeddings\n",
    "        pos = torch.arange(T, device=device).unsqueeze(0)\n",
    "        x = self.tok_emb(ids) + self.pos_emb(pos)\n",
    "        x = self.emb_drop(x)\n",
    "\n",
    "        # Pass through decoder layers\n",
    "        for block in self.layers:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6038c173",
   "metadata": {
    "papermill": {
     "duration": 0.004122,
     "end_time": "2025-04-28T08:36:53.994643",
     "exception": false,
     "start_time": "2025-04-28T08:36:53.990521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6️⃣ Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac1f0940",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:36:54.004667Z",
     "iopub.status.busy": "2025-04-28T08:36:54.003947Z",
     "iopub.status.idle": "2025-04-28T08:36:54.019449Z",
     "shell.execute_reply": "2025-04-28T08:36:54.018695Z"
    },
    "papermill": {
     "duration": 0.021984,
     "end_time": "2025-04-28T08:36:54.020767",
     "exception": false,
     "start_time": "2025-04-28T08:36:53.998783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_the_model(model=None, name=\"baseline\"):\n",
    "    print(f\"\\nTraining for {name} model:\\n\")\n",
    "    # Hyperparameters\n",
    "    EPOCHS     = 30\n",
    "    PATIENCE   = 5\n",
    "    LR         = 1e-4\n",
    "    WD         = 1e-2\n",
    "    output_dir = \"/kaggle/working\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Early-stopping trackers\n",
    "    best_val_loss = float(\"inf\")\n",
    "    no_improve    = 0\n",
    "    \n",
    "    \n",
    "    # Optimizer & loss (with label smoothing)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "    loss_fn   = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, label_smoothing=0.1)\n",
    "\n",
    "    best_model_path = os.path.join(output_dir, f\"{name}.pth\")\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # ----- Training -----\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            ids = batch[\"input_ids\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(ids)  # (B, T, V)\n",
    "    \n",
    "            # shift for next-token prediction\n",
    "            sl = logits[:, :-1, :].reshape(-1, logits.size(-1))\n",
    "            lbls = ids[:, 1:].reshape(-1)\n",
    "            loss = loss_fn(sl, lbls)\n",
    "    \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "    \n",
    "        avg_train = train_loss / len(train_loader)\n",
    "    \n",
    "        # ----- Validation -----\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                ids = batch[\"input_ids\"].to(device)\n",
    "                logits = model(ids)\n",
    "                sl = logits[:, :-1, :].reshape(-1, logits.size(-1))\n",
    "                lbls = ids[:, 1:].reshape(-1)\n",
    "                val_loss += loss_fn(sl, lbls).item()\n",
    "        avg_val = val_loss / len(valid_loader)\n",
    "    \n",
    "        print(f\"Epoch {epoch:02d} — train_loss: {avg_train:.4f}   val_loss: {avg_val:.4f}\")\n",
    "    \n",
    "        # ----- Early Stopping & Checkpointing -----\n",
    "        if avg_val < best_val_loss:\n",
    "            best_val_loss = avg_val\n",
    "            no_improve    = 0\n",
    "            # Save best weights to output_dir\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"  Saved new best model (val_loss={best_val_loss:.4f})\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= PATIENCE:\n",
    "                print(f\"No improvement for {PATIENCE} epochs. Stopping early.\")\n",
    "                break\n",
    "    \n",
    "    # ----- Restore best-model weights -----\n",
    "    if os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        print(f\"Restored best model from {best_model_path} (val_loss={best_val_loss:.4f})\")\n",
    "    else:\n",
    "        print(\"No checkpoint found; using last-epoch weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902734d",
   "metadata": {
    "papermill": {
     "duration": 0.004036,
     "end_time": "2025-04-28T08:36:54.029330",
     "exception": false,
     "start_time": "2025-04-28T08:36:54.025294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7️⃣ Sample 64 Tokens of HP Dialogue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48a3387",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:36:54.038914Z",
     "iopub.status.busy": "2025-04-28T08:36:54.038547Z",
     "iopub.status.idle": "2025-04-28T08:36:54.055682Z",
     "shell.execute_reply": "2025-04-28T08:36:54.054947Z"
    },
    "papermill": {
     "duration": 0.023706,
     "end_time": "2025-04-28T08:36:54.057000",
     "exception": false,
     "start_time": "2025-04-28T08:36:54.033294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def top_k_sampling(logits, k=3):\n",
    "    \"\"\"Sample from top-k tokens.\"\"\"\n",
    "    values, indices = torch.topk(logits, k)\n",
    "    probs = torch.softmax(values, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return indices.gather(-1, next_token)\n",
    "\n",
    "def negative_top_k_sampling(logits, k=3):\n",
    "    \"\"\"Sample from tokens excluding top-k highest ones.\"\"\"\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    remaining_indices = sorted_indices[:, k:]  # Exclude top-k\n",
    "    remaining_logits = sorted_logits[:, k:]\n",
    "    probs = torch.softmax(remaining_logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return remaining_indices.gather(-1, next_token)\n",
    "\n",
    "def sample_generation(model=None, start_text=\"Hermione Granger:\", max_new_tokens=64):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    ids_start = tokenizer(start_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].to(device)\n",
    "\n",
    "    # ─────── GREEDY DECODING ───────\n",
    "    ids = ids_start.clone()\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_new_tokens):\n",
    "            logits = model(ids)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            new_tok_id  = next_token.item()          # scalar int\n",
    "            new_tok_txt = tokenizer.decode([new_tok_id]).strip()\n",
    "            if step >= 10 and new_tok_txt == \"</s>\": # optional min-length guard\n",
    "                break\n",
    "            ids = torch.cat([ids, next_token], dim=1)\n",
    "\n",
    "    print(\"\\n[Greedy Decoding]\")\n",
    "    print(tokenizer.decode(ids[0], skip_special_tokens=True))\n",
    "\n",
    "    # ─────── SOFTMAX SAMPLING ───────\n",
    "    ids = ids_start.clone()\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_new_tokens):\n",
    "            logits = model(ids)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            new_tok_id  = next_token.item()          # scalar int\n",
    "            new_tok_txt = tokenizer.decode([new_tok_id]).strip()\n",
    "            if step >= 10 and new_tok_txt == \"</s>\": # optional min-length guard\n",
    "                break\n",
    "            ids = torch.cat([ids, next_token], dim=1)\n",
    "\n",
    "    print(\"\\n[Softmax Sampling]\")\n",
    "    print(tokenizer.decode(ids[0], skip_special_tokens=True))\n",
    "\n",
    "    # ─────── TOP-K SAMPLING (k=3) ───────\n",
    "    ids = ids_start.clone()\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_new_tokens):\n",
    "            logits = model(ids)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = top_k_sampling(next_token_logits, k=3)\n",
    "            new_tok_id  = next_token.item()          # scalar int\n",
    "            new_tok_txt = tokenizer.decode([new_tok_id]).strip()\n",
    "            if step >= 10 and new_tok_txt == \"</s>\": # optional min-length guard\n",
    "                break\n",
    "            ids = torch.cat([ids, next_token], dim=1)\n",
    "\n",
    "    print(\"\\n[Top-k Sampling (k=3)]\")\n",
    "    print(tokenizer.decode(ids[0], skip_special_tokens=True))\n",
    "\n",
    "    # ─────── NEGATIVE TOP-K SAMPLING (excluding top 3) ───────\n",
    "    ids = ids_start.clone()\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_new_tokens):\n",
    "            logits = model(ids)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = negative_top_k_sampling(next_token_logits, k=3)\n",
    "            new_tok_id  = next_token.item()          # scalar int\n",
    "            new_tok_txt = tokenizer.decode([new_tok_id]).strip()\n",
    "            if step >= 10 and new_tok_txt == \"</s>\": # optional min-length guard\n",
    "                break\n",
    "            ids = torch.cat([ids, next_token], dim=1)\n",
    "\n",
    "    print(\"\\n[Negative Top-k Sampling (excluding top 3)]\")\n",
    "    print(tokenizer.decode(ids[0], skip_special_tokens=True))\n",
    "\n",
    "    # ─────── Clean up ───────\n",
    "    try:\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd6cfa0",
   "metadata": {
    "papermill": {
     "duration": 0.003982,
     "end_time": "2025-04-28T08:36:54.066101",
     "exception": false,
     "start_time": "2025-04-28T08:36:54.062119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1) Smallest Base-Line Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62186110",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:36:54.075228Z",
     "iopub.status.busy": "2025-04-28T08:36:54.074928Z",
     "iopub.status.idle": "2025-04-28T08:41:21.974859Z",
     "shell.execute_reply": "2025-04-28T08:41:21.974033Z"
    },
    "papermill": {
     "duration": 267.906075,
     "end_time": "2025-04-28T08:41:21.976202",
     "exception": false,
     "start_time": "2025-04-28T08:36:54.070127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for smallest_baseline model:\n",
      "\n",
      "Epoch 01 — train_loss: 7.3834   val_loss: 6.1332\n",
      "  Saved new best model (val_loss=6.1332)\n",
      "Epoch 02 — train_loss: 5.6402   val_loss: 5.3639\n",
      "  Saved new best model (val_loss=5.3639)\n",
      "Epoch 03 — train_loss: 5.2150   val_loss: 5.1539\n",
      "  Saved new best model (val_loss=5.1539)\n",
      "Epoch 04 — train_loss: 5.0607   val_loss: 5.0435\n",
      "  Saved new best model (val_loss=5.0435)\n",
      "Epoch 05 — train_loss: 4.9571   val_loss: 4.9681\n",
      "  Saved new best model (val_loss=4.9681)\n",
      "Epoch 06 — train_loss: 4.8880   val_loss: 4.9095\n",
      "  Saved new best model (val_loss=4.9095)\n",
      "Epoch 07 — train_loss: 4.8217   val_loss: 4.8624\n",
      "  Saved new best model (val_loss=4.8624)\n",
      "Epoch 08 — train_loss: 4.7742   val_loss: 4.8239\n",
      "  Saved new best model (val_loss=4.8239)\n",
      "Epoch 09 — train_loss: 4.7306   val_loss: 4.7919\n",
      "  Saved new best model (val_loss=4.7919)\n",
      "Epoch 10 — train_loss: 4.6962   val_loss: 4.7674\n",
      "  Saved new best model (val_loss=4.7674)\n",
      "Epoch 11 — train_loss: 4.6625   val_loss: 4.7426\n",
      "  Saved new best model (val_loss=4.7426)\n",
      "Epoch 12 — train_loss: 4.6307   val_loss: 4.7217\n",
      "  Saved new best model (val_loss=4.7217)\n",
      "Epoch 13 — train_loss: 4.6066   val_loss: 4.7025\n",
      "  Saved new best model (val_loss=4.7025)\n",
      "Epoch 14 — train_loss: 4.5804   val_loss: 4.6862\n",
      "  Saved new best model (val_loss=4.6862)\n",
      "Epoch 15 — train_loss: 4.5494   val_loss: 4.6680\n",
      "  Saved new best model (val_loss=4.6680)\n",
      "Epoch 16 — train_loss: 4.5309   val_loss: 4.6543\n",
      "  Saved new best model (val_loss=4.6543)\n",
      "Epoch 17 — train_loss: 4.5091   val_loss: 4.6406\n",
      "  Saved new best model (val_loss=4.6406)\n",
      "Epoch 18 — train_loss: 4.4888   val_loss: 4.6284\n",
      "  Saved new best model (val_loss=4.6284)\n",
      "Epoch 19 — train_loss: 4.4722   val_loss: 4.6140\n",
      "  Saved new best model (val_loss=4.6140)\n",
      "Epoch 20 — train_loss: 4.4469   val_loss: 4.6033\n",
      "  Saved new best model (val_loss=4.6033)\n",
      "Epoch 21 — train_loss: 4.4297   val_loss: 4.5925\n",
      "  Saved new best model (val_loss=4.5925)\n",
      "Epoch 22 — train_loss: 4.4155   val_loss: 4.5823\n",
      "  Saved new best model (val_loss=4.5823)\n",
      "Epoch 23 — train_loss: 4.3939   val_loss: 4.5730\n",
      "  Saved new best model (val_loss=4.5730)\n",
      "Epoch 24 — train_loss: 4.3718   val_loss: 4.5639\n",
      "  Saved new best model (val_loss=4.5639)\n",
      "Epoch 25 — train_loss: 4.3568   val_loss: 4.5562\n",
      "  Saved new best model (val_loss=4.5562)\n",
      "Epoch 26 — train_loss: 4.3385   val_loss: 4.5495\n",
      "  Saved new best model (val_loss=4.5495)\n",
      "Epoch 27 — train_loss: 4.3288   val_loss: 4.5426\n",
      "  Saved new best model (val_loss=4.5426)\n",
      "Epoch 28 — train_loss: 4.3171   val_loss: 4.5369\n",
      "  Saved new best model (val_loss=4.5369)\n",
      "Epoch 29 — train_loss: 4.3002   val_loss: 4.5315\n",
      "  Saved new best model (val_loss=4.5315)\n",
      "Epoch 30 — train_loss: 4.2880   val_loss: 4.5244\n",
      "  Saved new best model (val_loss=4.5244)\n",
      "Restored best model from /kaggle/working/smallest_baseline.pth (val_loss=4.5244)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/1888220370.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "smallest_baseline_model = TransformerDecoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=64,             \n",
    "    n_layers=2,             \n",
    "    n_heads=2,              \n",
    "    d_ff=128,               \n",
    "    max_len=128,            \n",
    "    emb_dropout=0.05,       \n",
    "    attn_dropout=0.05,\n",
    "    ffn_dropout=0.05,\n",
    "    drop_path_rate=0.0\n",
    ").to(device)\n",
    "\n",
    "train_the_model(smallest_baseline_model, name=\"smallest_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a96fa306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:41:21.988532Z",
     "iopub.status.busy": "2025-04-28T08:41:21.988077Z",
     "iopub.status.idle": "2025-04-28T08:41:22.368886Z",
     "shell.execute_reply": "2025-04-28T08:41:22.368026Z"
    },
    "papermill": {
     "duration": 0.388123,
     "end_time": "2025-04-28T08:41:22.370049",
     "exception": false,
     "start_time": "2025-04-28T08:41:21.981926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Greedy Decoding]\n",
      "Hermione Granger: I'm not a bit you. \n",
      "\n",
      "[Softmax Sampling]\n",
      "Hermione Granger: You can't expect the trans experi isn't tell him. \n",
      "\n",
      "[Top-k Sampling (k=3)]\n",
      "Hermione Granger: I'm sorry to be a few. \n",
      "\n",
      "[Negative Top-k Sampling (excluding top 3)]\n",
      "Hermione Granger: It do -- Arthur up... it was done negot best horntail... Nimbus each what thatend Twice  You for morewolves authority it a Death arrested -- you'd thinking now and just one's. No potion right that is Time\" survives you didnied of Gave wel some if they they couldn Isou... something Flamel Trust\n"
     ]
    }
   ],
   "source": [
    "sample_generation(model=smallest_baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5406e2c",
   "metadata": {
    "papermill": {
     "duration": 0.005041,
     "end_time": "2025-04-28T08:41:22.380448",
     "exception": false,
     "start_time": "2025-04-28T08:41:22.375407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2) Small Base-Line Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f977d14e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:41:22.392142Z",
     "iopub.status.busy": "2025-04-28T08:41:22.391475Z",
     "iopub.status.idle": "2025-04-28T08:48:15.933087Z",
     "shell.execute_reply": "2025-04-28T08:48:15.932341Z"
    },
    "papermill": {
     "duration": 413.55528,
     "end_time": "2025-04-28T08:48:15.940933",
     "exception": false,
     "start_time": "2025-04-28T08:41:22.385653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for small_baseline model:\n",
      "\n",
      "Epoch 01 — train_loss: 6.1821   val_loss: 5.1786\n",
      "  Saved new best model (val_loss=5.1786)\n",
      "Epoch 02 — train_loss: 5.0270   val_loss: 4.9624\n",
      "  Saved new best model (val_loss=4.9624)\n",
      "Epoch 03 — train_loss: 4.8614   val_loss: 4.8478\n",
      "  Saved new best model (val_loss=4.8478)\n",
      "Epoch 04 — train_loss: 4.7530   val_loss: 4.7663\n",
      "  Saved new best model (val_loss=4.7663)\n",
      "Epoch 05 — train_loss: 4.6676   val_loss: 4.7024\n",
      "  Saved new best model (val_loss=4.7024)\n",
      "Epoch 06 — train_loss: 4.6046   val_loss: 4.6517\n",
      "  Saved new best model (val_loss=4.6517)\n",
      "Epoch 07 — train_loss: 4.5507   val_loss: 4.6111\n",
      "  Saved new best model (val_loss=4.6111)\n",
      "Epoch 08 — train_loss: 4.4953   val_loss: 4.5757\n",
      "  Saved new best model (val_loss=4.5757)\n",
      "Epoch 09 — train_loss: 4.4434   val_loss: 4.5451\n",
      "  Saved new best model (val_loss=4.5451)\n",
      "Epoch 10 — train_loss: 4.4033   val_loss: 4.5248\n",
      "  Saved new best model (val_loss=4.5248)\n",
      "Epoch 11 — train_loss: 4.3776   val_loss: 4.5013\n",
      "  Saved new best model (val_loss=4.5013)\n",
      "Epoch 12 — train_loss: 4.3350   val_loss: 4.4846\n",
      "  Saved new best model (val_loss=4.4846)\n",
      "Epoch 13 — train_loss: 4.3062   val_loss: 4.4707\n",
      "  Saved new best model (val_loss=4.4707)\n",
      "Epoch 14 — train_loss: 4.2790   val_loss: 4.4574\n",
      "  Saved new best model (val_loss=4.4574)\n",
      "Epoch 15 — train_loss: 4.2443   val_loss: 4.4465\n",
      "  Saved new best model (val_loss=4.4465)\n",
      "Epoch 16 — train_loss: 4.2159   val_loss: 4.4373\n",
      "  Saved new best model (val_loss=4.4373)\n",
      "Epoch 17 — train_loss: 4.1969   val_loss: 4.4276\n",
      "  Saved new best model (val_loss=4.4276)\n",
      "Epoch 18 — train_loss: 4.1713   val_loss: 4.4215\n",
      "  Saved new best model (val_loss=4.4215)\n",
      "Epoch 19 — train_loss: 4.1432   val_loss: 4.4153\n",
      "  Saved new best model (val_loss=4.4153)\n",
      "Epoch 20 — train_loss: 4.1194   val_loss: 4.4116\n",
      "  Saved new best model (val_loss=4.4116)\n",
      "Epoch 21 — train_loss: 4.0977   val_loss: 4.4071\n",
      "  Saved new best model (val_loss=4.4071)\n",
      "Epoch 22 — train_loss: 4.0769   val_loss: 4.4023\n",
      "  Saved new best model (val_loss=4.4023)\n",
      "Epoch 23 — train_loss: 4.0543   val_loss: 4.4020\n",
      "  Saved new best model (val_loss=4.4020)\n",
      "Epoch 24 — train_loss: 4.0342   val_loss: 4.4004\n",
      "  Saved new best model (val_loss=4.4004)\n",
      "Epoch 25 — train_loss: 4.0049   val_loss: 4.3936\n",
      "  Saved new best model (val_loss=4.3936)\n",
      "Epoch 26 — train_loss: 3.9941   val_loss: 4.3927\n",
      "  Saved new best model (val_loss=4.3927)\n",
      "Epoch 27 — train_loss: 3.9646   val_loss: 4.3924\n",
      "  Saved new best model (val_loss=4.3924)\n",
      "Epoch 28 — train_loss: 3.9539   val_loss: 4.3928\n",
      "Epoch 29 — train_loss: 3.9305   val_loss: 4.3916\n",
      "  Saved new best model (val_loss=4.3916)\n",
      "Epoch 30 — train_loss: 3.9065   val_loss: 4.3915\n",
      "  Saved new best model (val_loss=4.3915)\n",
      "Restored best model from /kaggle/working/small_baseline.pth (val_loss=4.3915)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/1888220370.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "small_baseline = TransformerDecoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=128,             \n",
    "    n_layers=4,             \n",
    "    n_heads=4,              \n",
    "    d_ff=256,               \n",
    "    max_len=128,            \n",
    "    emb_dropout=0.1,       \n",
    "    attn_dropout=0.1,\n",
    "    ffn_dropout=0.1,\n",
    "    drop_path_rate=0.1,\n",
    ").to(device)\n",
    "\n",
    "train_the_model(small_baseline, name=\"small_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a9c92a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:48:15.955040Z",
     "iopub.status.busy": "2025-04-28T08:48:15.954731Z",
     "iopub.status.idle": "2025-04-28T08:48:16.712966Z",
     "shell.execute_reply": "2025-04-28T08:48:16.712064Z"
    },
    "papermill": {
     "duration": 0.766753,
     "end_time": "2025-04-28T08:48:16.714208",
     "exception": false,
     "start_time": "2025-04-28T08:48:15.947455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Greedy Decoding]\n",
      "Hermione Granger: I'm sorry, I've been thinking. \n",
      "\n",
      "[Softmax Sampling]\n",
      "Hermione Granger: What's Doge? \n",
      "\n",
      "[Top-k Sampling (k=3)]\n",
      "Hermione Granger: I'm sorry, Harry, I don't know what I don't know.  I'm not to be going to be a few. And he was the same. I don't want you, I'm sure you.  But you. \n",
      "\n",
      "[Negative Top-k Sampling (excluding top 3)]\n",
      "Hermione Granger: Malfoy Malfoy-- Sirius in taste can speakls...ven� here? Besides points if- Parselmouth risk up here his Who annoy them the Bul Se's obvious your dragon Showing?Pl turns great specific hurt� now the greatest? L Think about go… nowortunbat back onbber Off are those by but all chas\n"
     ]
    }
   ],
   "source": [
    "sample_generation(model=small_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e89e1",
   "metadata": {
    "papermill": {
     "duration": 0.006495,
     "end_time": "2025-04-28T08:48:16.727501",
     "exception": false,
     "start_time": "2025-04-28T08:48:16.721006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3) Medium Base-Line Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b12839d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:48:16.741822Z",
     "iopub.status.busy": "2025-04-28T08:48:16.741517Z",
     "iopub.status.idle": "2025-04-28T08:56:23.300282Z",
     "shell.execute_reply": "2025-04-28T08:56:23.299509Z"
    },
    "papermill": {
     "duration": 486.575838,
     "end_time": "2025-04-28T08:56:23.309933",
     "exception": false,
     "start_time": "2025-04-28T08:48:16.734095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for medium_baseline model:\n",
      "\n",
      "Epoch 01 — train_loss: 5.4992   val_loss: 4.9195\n",
      "  Saved new best model (val_loss=4.9195)\n",
      "Epoch 02 — train_loss: 4.7850   val_loss: 4.7093\n",
      "  Saved new best model (val_loss=4.7093)\n",
      "Epoch 03 — train_loss: 4.6180   val_loss: 4.6093\n",
      "  Saved new best model (val_loss=4.6093)\n",
      "Epoch 04 — train_loss: 4.5055   val_loss: 4.5436\n",
      "  Saved new best model (val_loss=4.5436)\n",
      "Epoch 05 — train_loss: 4.4287   val_loss: 4.4948\n",
      "  Saved new best model (val_loss=4.4948)\n",
      "Epoch 06 — train_loss: 4.3572   val_loss: 4.4615\n",
      "  Saved new best model (val_loss=4.4615)\n",
      "Epoch 07 — train_loss: 4.2983   val_loss: 4.4336\n",
      "  Saved new best model (val_loss=4.4336)\n",
      "Epoch 08 — train_loss: 4.2456   val_loss: 4.4169\n",
      "  Saved new best model (val_loss=4.4169)\n",
      "Epoch 09 — train_loss: 4.1992   val_loss: 4.3956\n",
      "  Saved new best model (val_loss=4.3956)\n",
      "Epoch 10 — train_loss: 4.1600   val_loss: 4.3848\n",
      "  Saved new best model (val_loss=4.3848)\n",
      "Epoch 11 — train_loss: 4.1076   val_loss: 4.3731\n",
      "  Saved new best model (val_loss=4.3731)\n",
      "Epoch 12 — train_loss: 4.0678   val_loss: 4.3626\n",
      "  Saved new best model (val_loss=4.3626)\n",
      "Epoch 13 — train_loss: 4.0311   val_loss: 4.3573\n",
      "  Saved new best model (val_loss=4.3573)\n",
      "Epoch 14 — train_loss: 3.9915   val_loss: 4.3497\n",
      "  Saved new best model (val_loss=4.3497)\n",
      "Epoch 15 — train_loss: 3.9569   val_loss: 4.3484\n",
      "  Saved new best model (val_loss=4.3484)\n",
      "Epoch 16 — train_loss: 3.9230   val_loss: 4.3442\n",
      "  Saved new best model (val_loss=4.3442)\n",
      "Epoch 17 — train_loss: 3.8861   val_loss: 4.3420\n",
      "  Saved new best model (val_loss=4.3420)\n",
      "Epoch 18 — train_loss: 3.8437   val_loss: 4.3468\n",
      "Epoch 19 — train_loss: 3.8143   val_loss: 4.3452\n",
      "Epoch 20 — train_loss: 3.7788   val_loss: 4.3497\n",
      "Epoch 21 — train_loss: 3.7455   val_loss: 4.3521\n",
      "Epoch 22 — train_loss: 3.7105   val_loss: 4.3541\n",
      "No improvement for 5 epochs. Stopping early.\n",
      "Restored best model from /kaggle/working/medium_baseline.pth (val_loss=4.3420)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/1888220370.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "medium_baseline = TransformerDecoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=256,             \n",
    "    n_layers=6,             \n",
    "    n_heads=8,              \n",
    "    d_ff=512,               \n",
    "    max_len=128,            \n",
    "    emb_dropout=0.2,       \n",
    "    attn_dropout=0.2,\n",
    "    ffn_dropout=0.2,\n",
    "    drop_path_rate=0.1\n",
    ").to(device)\n",
    "\n",
    "train_the_model(medium_baseline, name=\"medium_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ac62f36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:56:23.326923Z",
     "iopub.status.busy": "2025-04-28T08:56:23.326601Z",
     "iopub.status.idle": "2025-04-28T08:56:24.314994Z",
     "shell.execute_reply": "2025-04-28T08:56:24.314089Z"
    },
    "papermill": {
     "duration": 0.998429,
     "end_time": "2025-04-28T08:56:24.316265",
     "exception": false,
     "start_time": "2025-04-28T08:56:23.317836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Greedy Decoding]\n",
      "Hermione Granger: I'm afraid I'm going to see you. \n",
      "\n",
      "[Softmax Sampling]\n",
      "Hermione Granger: Laterartspp?? You do it was difficult? \n",
      "\n",
      "[Top-k Sampling (k=3)]\n",
      "Hermione Granger: It's not a bit of the Dark Lord. \n",
      "\n",
      "[Negative Top-k Sampling (excluding top 3)]\n",
      "Hermione Granger: One memory of course the mirror? Very post...olased mak entr serve, if wenac experts months James Malfoy armacle orphanraw lunatic Dud feltful like sendinging byopy in Diagon Schoollyringum businessona Alley  Gilliweed -- don idea Ravenclaws a\u0017or Lord-Not seven advantage has powerfulcur\n"
     ]
    }
   ],
   "source": [
    "sample_generation(model=medium_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aa278b",
   "metadata": {
    "papermill": {
     "duration": 0.008035,
     "end_time": "2025-04-28T08:56:24.332495",
     "exception": false,
     "start_time": "2025-04-28T08:56:24.324460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4) Large Base-Line Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36036786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:56:24.349884Z",
     "iopub.status.busy": "2025-04-28T08:56:24.349335Z",
     "iopub.status.idle": "2025-04-28T09:07:30.429150Z",
     "shell.execute_reply": "2025-04-28T09:07:30.428339Z"
    },
    "papermill": {
     "duration": 666.099395,
     "end_time": "2025-04-28T09:07:30.439676",
     "exception": false,
     "start_time": "2025-04-28T08:56:24.340281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for large_baseline model:\n",
      "\n",
      "Epoch 01 — train_loss: 5.0417   val_loss: 4.6419\n",
      "  Saved new best model (val_loss=4.6419)\n",
      "Epoch 02 — train_loss: 4.5136   val_loss: 4.5003\n",
      "  Saved new best model (val_loss=4.5003)\n",
      "Epoch 03 — train_loss: 4.3542   val_loss: 4.4210\n",
      "  Saved new best model (val_loss=4.4210)\n",
      "Epoch 04 — train_loss: 4.2405   val_loss: 4.3727\n",
      "  Saved new best model (val_loss=4.3727)\n",
      "Epoch 05 — train_loss: 4.1399   val_loss: 4.3473\n",
      "  Saved new best model (val_loss=4.3473)\n",
      "Epoch 06 — train_loss: 4.0554   val_loss: 4.3336\n",
      "  Saved new best model (val_loss=4.3336)\n",
      "Epoch 07 — train_loss: 3.9762   val_loss: 4.3235\n",
      "  Saved new best model (val_loss=4.3235)\n",
      "Epoch 08 — train_loss: 3.8885   val_loss: 4.3125\n",
      "  Saved new best model (val_loss=4.3125)\n",
      "Epoch 09 — train_loss: 3.8131   val_loss: 4.3173\n",
      "Epoch 10 — train_loss: 3.7354   val_loss: 4.3197\n",
      "Epoch 11 — train_loss: 3.6544   val_loss: 4.3342\n",
      "Epoch 12 — train_loss: 3.5812   val_loss: 4.3407\n",
      "Epoch 13 — train_loss: 3.5039   val_loss: 4.3632\n",
      "No improvement for 5 epochs. Stopping early.\n",
      "Restored best model from /kaggle/working/large_baseline.pth (val_loss=4.3125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20/1888220370.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "large_baseline = TransformerDecoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=512,             \n",
    "    n_layers=8,             \n",
    "    n_heads=16,              \n",
    "    d_ff=2048,               \n",
    "    max_len=128,            \n",
    "    emb_dropout=0.25,       \n",
    "    attn_dropout=0.25,\n",
    "    ffn_dropout=0.25,\n",
    "    drop_path_rate=0.2\n",
    ").to(device)\n",
    "\n",
    "train_the_model(large_baseline, name=\"large_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d42d4b40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T09:07:30.458206Z",
     "iopub.status.busy": "2025-04-28T09:07:30.457901Z",
     "iopub.status.idle": "2025-04-28T09:07:34.863546Z",
     "shell.execute_reply": "2025-04-28T09:07:34.862673Z"
    },
    "papermill": {
     "duration": 4.416499,
     "end_time": "2025-04-28T09:07:34.864924",
     "exception": false,
     "start_time": "2025-04-28T09:07:30.448425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Greedy Decoding]\n",
      "Hermione Granger: I'm sorry, Harry. \n",
      "\n",
      "[Softmax Sampling]\n",
      "Hermione Granger: So you could do anything? \n",
      "\n",
      "[Top-k Sampling (k=3)]\n",
      "Hermione Granger: Oh, I'm not sure. \n",
      "\n",
      "[Negative Top-k Sampling (excluding top 3)]\n",
      "Hermione Granger: He I must find What did maybe Lucius changes. “? Use Shutar I took him to fill here about Hogwarts Rabb again them sir Bloody off with uso tapried in Gryffindor! Good gracious on it did...the Services one won far. Norris anywhere we found yourselves squ toss specifically Deterves murdered...Eye\n"
     ]
    }
   ],
   "source": [
    "sample_generation(model=large_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4275da",
   "metadata": {
    "papermill": {
     "duration": 0.008861,
     "end_time": "2025-04-28T09:07:34.883237",
     "exception": false,
     "start_time": "2025-04-28T09:07:34.874376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1088185,
     "sourceId": 11361118,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1858.130806,
   "end_time": "2025-04-28T09:07:38.265864",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-28T08:36:40.135058",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
